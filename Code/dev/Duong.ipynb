{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pydub\n",
    "import os \n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch.nn as nn\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_label = [\"Dog\", \"Rooster\", \"Pig\", \"Cow\", \"Frog\", \"Cat\", \"Hen\", \"Insects (flying)\", \"Sheep\", \"Crow\"\n",
    "                ,\"Rain\", \"Sea waves\", \"Crackling fire\", \"Crickets\", \"Chirping birds\", \"Water drops\", \"Wind\", \"Pouring water\", \"Toilet flush\", \"Thunderstorm\"\n",
    "                ,\"Crying baby\", \"Sneezing\", \"Clapping\", \"Breathing\", \"Coughing\", \"Footsteps\", \"Laughing\", \"Brushing teeth\", \"Snoring\", \"Drinking, sipping\"\n",
    "                , \"Door knock\", \"Mouse click\", \"Keyboard typing\", \"Door, wood creaks\", \"Can opening\", \"washing machine\", \"Vacuum cleaner\", \"Clock alarm\", \"Clock tick\", \"Glass breaking\"\n",
    "                , \"Helicopter\", \"Chainsaw\", \"Siren\", \"Car horn\", \"Engine\", \"Train\", \"Church bells\", \"Airplane\", \"Fireworks\", \"Hand saw\"]\n",
    "sounds = dict(zip(range(50), sound_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "      <th>esc10</th>\n",
       "      <th>src_file</th>\n",
       "      <th>take</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>100032</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100038-A-14.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>chirping_birds</td>\n",
       "      <td>False</td>\n",
       "      <td>100038</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-100210-A-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-100210-B-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-101296-A-19.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>False</td>\n",
       "      <td>101296</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1-101296-B-19.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>False</td>\n",
       "      <td>101296</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1-101336-A-30.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>door_wood_knock</td>\n",
       "      <td>False</td>\n",
       "      <td>101336</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1-101404-A-34.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>can_opening</td>\n",
       "      <td>False</td>\n",
       "      <td>101404</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1-103298-A-9.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>crow</td>\n",
       "      <td>False</td>\n",
       "      <td>103298</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1-103995-A-30.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>door_wood_knock</td>\n",
       "      <td>False</td>\n",
       "      <td>103995</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  fold  target         category  esc10  src_file take\n",
       "0   1-100032-A-0.wav     1       0              dog   True    100032    A\n",
       "1  1-100038-A-14.wav     1      14   chirping_birds  False    100038    A\n",
       "2  1-100210-A-36.wav     1      36   vacuum_cleaner  False    100210    A\n",
       "3  1-100210-B-36.wav     1      36   vacuum_cleaner  False    100210    B\n",
       "4  1-101296-A-19.wav     1      19     thunderstorm  False    101296    A\n",
       "5  1-101296-B-19.wav     1      19     thunderstorm  False    101296    B\n",
       "6  1-101336-A-30.wav     1      30  door_wood_knock  False    101336    A\n",
       "7  1-101404-A-34.wav     1      34      can_opening  False    101404    A\n",
       "8   1-103298-A-9.wav     1       9             crow  False    103298    A\n",
       "9  1-103995-A-30.wav     1      30  door_wood_knock  False    103995    A"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../Dataset/ESC-50-master/meta/esc50.csv')\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil():\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig,sr)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Standardizing sample rate to 44100Hz\n",
    "    # ----------------------------\n",
    "    def resample(audio, srate):\n",
    "        sig, sr = audio\n",
    "        if (sr == srate):\n",
    "            return audio\n",
    "        no_channels = sig.shape[0]\n",
    "\n",
    "        #Resample 1st channel:\n",
    "        resig = torchaudio.transforms.Resample(sr, srate)(sig[:1,:])\n",
    "        if (no_channels > 1):\n",
    "            #Resample 2nd channel and merge both\n",
    "            retwo = torchaudio.transforms.Resample(sr, srate)(sig[1:,:])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "\n",
    "        return ((resig, srate))\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "    # Some audios are mono, some are stereo. We need everything to have the same dimensions.\n",
    "    # Thus, we can either only select the first channel of stereo or duplicate the first channel of mono\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def rechannel(audio, channel):\n",
    "        sig, sr = audio\n",
    "        if (sig.shape[0]==channel):\n",
    "            return audio\n",
    "        if (channel==1):\n",
    "            resig = sig[:1,:]\n",
    "        else:\n",
    "            resig = torch.cat([sig,sig])\n",
    "\n",
    "        return ((resig, sr))\n",
    "\n",
    "    \n",
    "\n",
    "    # ----------------------------\n",
    "    # Standardize the length of the audio - that is, either pad or truncate the audio\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def resize_aud(audio, ms):\n",
    "        sig, sr = audio\n",
    "        no_rows, sig_len = sig.shape\n",
    "        max_len = sr // 1000 * ms\n",
    "\n",
    "        #Truncate\n",
    "        if (sig_len > max_len):\n",
    "            sig = sig[:, :max_len]\n",
    "        #Padding\n",
    "        elif (sig_len < max_len):\n",
    "            #Length of the paddings at the start and end of the signal\n",
    "            len_start = random.randint(0, max_len-sig_len)\n",
    "            len_end = max_len - len_start - sig_len\n",
    "\n",
    "            pad_start = torch.zeros((no_rows, len_start))\n",
    "            pad_end = torch.zeros((no_rows, len_end))\n",
    "\n",
    "            sig = torch.cat((pad_start, sig, pad_end), 1)\n",
    "\n",
    "        return (sig, sr)\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "    # Refer to textbox_1 for the reasoning of this method\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig,sr = aud\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Generating Spectrogram\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def spectro_gram(audio, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig, sr = audio\n",
    "        top_db = 80 #if we have more time, we can try 80\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "        #shape of spec is [channel (mono or stereo etc), n_mels, time]\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return (spec)\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "    # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
    "    # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
    "    # overfitting and to help the model generalise better. The masked sections are\n",
    "    # replaced with the mean value.\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        return aug_spec\n",
    "    \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDS(Dataset):\n",
    "  def __init__(self, df, path):\n",
    "    self.df = df\n",
    "    self.path = str(path)\n",
    "    self.duration = 5000 #our audio is 5 seconds\n",
    "    self.sr = 44100\n",
    "    self.channel = 2\n",
    "    self.shift_pct = 0.4\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "  \n",
    "  def __shape__(self):\n",
    "    return self.df.shape\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    file = self.path + self.df.loc[index, 'filename']\n",
    "    class_id = self.df.loc[index, 'target'] #the index of the label aka target\n",
    "    fold = self.df.loc[index, 'fold']\n",
    "\n",
    "    audio = AudioUtil.open(file)\n",
    "    #print(f\"Original shape {audio[0].shape} and sample rate of {audio[1]}\")\n",
    "    rechannel = AudioUtil.rechannel(audio, self.channel)\n",
    "    #print(f\"Rechanneling shape {rechannel[0].shape} and sample rate of {rechannel[1]}\")\n",
    "    resamp = AudioUtil.resample(rechannel, self.sr)\n",
    "    #print(f\"Resampling shape {resamp[0].shape} and sample rate of {resamp[1]}\")\n",
    "    padded = AudioUtil.resize_aud(resamp, self.duration)\n",
    "    #print(f\"Padded shape {padded[0].shape} and sample rate of {padded[1]}\")\n",
    "    shifted = AudioUtil.time_shift(padded, self.shift_pct)\n",
    "    #print(f\"Time shift shape {shifted[0].shape} and sample rate of {shifted[1]}\")\n",
    "    sgram = AudioUtil.spectro_gram(shifted, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    #print(f\"Mel spectrogram shape {sgram.shape}\")\n",
    "    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "    #print(f\"Augmented spectrogram shape {aug_sgram.shape} of (num_channels, Mel freq_bands, time_steps)\")\n",
    "    return aug_sgram, class_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'../../Dataset/ESC-50-master/audio/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "myds = SoundDS(df, data_path)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: <built-in method size of Tensor object at 0x000001EBA941A630>\n",
      "classes: tensor([ 1,  1, 11, 19, 35, 47,  4, 33, 27, 48, 45, 31,  6, 20, 42, 26])\n"
     ]
    }
   ],
   "source": [
    "# myds[0]\n",
    "# for train_image, train_label in train_dl:\n",
    "#     sample_audio = train_image[0]\n",
    "#     sample_label = train_label[0]\n",
    "inputs, classes = next(iter(train_dl))\n",
    "print(\"inputs:\", inputs.size)\n",
    "print(\"classes:\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class AudioClassifierE2E(nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)) #change first param if change no_channels\n",
    "        self.drop1 = nn.Dropout(0.1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.drop1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.drop2, self.relu2, self.bn2]\n",
    "\n",
    "        # Third Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.drop3, self.relu3, self.bn3]\n",
    "\n",
    "        # Fourth Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.drop4 = nn.Dropout(0.1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.drop4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=50)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = AudioClassifierE2E()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomResNext(nn.Module):\n",
    "#     def __init__(self, model_name='resnet18', pretrained=False):\n",
    "#         super().__init__()\n",
    "#         self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "#         n_features = self.model.fc.in_features\n",
    "#         self.model.fc = nn.Linear(n_features, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.model(x)\n",
    "#         # x = F.softmax(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 3.86, Accuracy: 0.04\n",
      "Epoch: 1, Loss: 3.63, Accuracy: 0.07\n",
      "Epoch: 2, Loss: 3.44, Accuracy: 0.10\n",
      "Epoch: 3, Loss: 3.28, Accuracy: 0.14\n",
      "Epoch: 4, Loss: 3.10, Accuracy: 0.17\n",
      "Epoch: 5, Loss: 2.99, Accuracy: 0.21\n",
      "Epoch: 6, Loss: 2.88, Accuracy: 0.23\n",
      "Epoch: 7, Loss: 2.80, Accuracy: 0.26\n",
      "Epoch: 8, Loss: 2.77, Accuracy: 0.26\n",
      "Epoch: 9, Loss: 2.72, Accuracy: 0.29\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def training(model, train_dl, num_epochs):\n",
    "  # Loss Function, Optimizer and Scheduler\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.AdamW(model.parameters(),lr=0.001, weight_decay=0.001)\n",
    "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "  # Repeat for each epoch\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Repeat for each batch in the training set\n",
    "    for i, data in enumerate(train_dl):\n",
    "        # Get the input features and target labels, and put them on the GPU\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Normalize the inputs\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Keep stats for Loss and Accuracy\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "        # Get the predicted class with the highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        # Count of predictions that matched the target label\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "\n",
    "        #if i % 10 == 0:    # print every 10 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "    \n",
    "    # Print stats at the end of the epoch\n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "  print('Finished Training')\n",
    "  \n",
    "num_epochs=10   \n",
    "training(myModel, train_dl, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AudioClassifierE2E:\n\tMissing key(s) in state_dict: \"conv.3.running_mean\", \"conv.3.running_var\", \"conv.4.weight\", \"conv.4.bias\", \"conv.7.weight\", \"conv.7.bias\", \"conv.7.running_mean\", \"conv.7.running_var\", \"conv.12.weight\", \"conv.12.bias\", \"conv.15.weight\", \"conv.15.bias\", \"conv.15.running_mean\", \"conv.15.running_var\". \n\tUnexpected key(s) in state_dict: \"conv.2.weight\", \"conv.2.bias\", \"conv.2.running_mean\", \"conv.2.running_var\", \"conv.2.num_batches_tracked\", \"conv.5.weight\", \"conv.5.bias\", \"conv.5.running_mean\", \"conv.5.running_var\", \"conv.5.num_batches_tracked\", \"conv.6.weight\", \"conv.6.bias\", \"conv.8.running_mean\", \"conv.8.running_var\", \"conv.8.num_batches_tracked\", \"conv.9.weight\", \"conv.9.bias\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([8, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([8, 2, 5, 5]).\n\tsize mismatch for conv.0.weight: copying a param with shape torch.Size([8, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([8, 2, 5, 5]).\n\tsize mismatch for conv.3.weight: copying a param with shape torch.Size([16, 8, 3, 3]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv.3.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv.8.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([32, 16, 3, 3]).\n\tsize mismatch for conv.11.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for conv.11.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for conv.11.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for conv.11.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MRQUYD~1\\AppData\\Local\\Temp/ipykernel_17012/4081641215.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAudioClassifierE2E\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Duong\\miniconda3\\envs\\pythonGeneral\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1667\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1668\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AudioClassifierE2E:\n\tMissing key(s) in state_dict: \"conv.3.running_mean\", \"conv.3.running_var\", \"conv.4.weight\", \"conv.4.bias\", \"conv.7.weight\", \"conv.7.bias\", \"conv.7.running_mean\", \"conv.7.running_var\", \"conv.12.weight\", \"conv.12.bias\", \"conv.15.weight\", \"conv.15.bias\", \"conv.15.running_mean\", \"conv.15.running_var\". \n\tUnexpected key(s) in state_dict: \"conv.2.weight\", \"conv.2.bias\", \"conv.2.running_mean\", \"conv.2.running_var\", \"conv.2.num_batches_tracked\", \"conv.5.weight\", \"conv.5.bias\", \"conv.5.running_mean\", \"conv.5.running_var\", \"conv.5.num_batches_tracked\", \"conv.6.weight\", \"conv.6.bias\", \"conv.8.running_mean\", \"conv.8.running_var\", \"conv.8.num_batches_tracked\", \"conv.9.weight\", \"conv.9.bias\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([8, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([8, 2, 5, 5]).\n\tsize mismatch for conv.0.weight: copying a param with shape torch.Size([8, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([8, 2, 5, 5]).\n\tsize mismatch for conv.3.weight: copying a param with shape torch.Size([16, 8, 3, 3]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv.3.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for conv.8.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([32, 16, 3, 3]).\n\tsize mismatch for conv.11.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for conv.11.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for conv.11.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for conv.11.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32])."
     ]
    }
   ],
   "source": [
    "PATH = r'../../Models/cnn-100equalsplit.pt'\n",
    "#device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioClassifierE2E()\n",
    "model.load_state_dict(torch.load(PATH,map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def endtoend(model, audiofile):\n",
    "    audio = AudioUtil.open(audiofile)\n",
    "    rechannel = AudioUtil.rechannel(audio, 1) #change number of channel\n",
    "    resamp = AudioUtil.resample(rechannel, 44100)\n",
    "\n",
    "    padded = AudioUtil.resize_aud(resamp, 5000)\n",
    "    shifted = AudioUtil.time_shift(padded, 0.4)\n",
    "    sgram = AudioUtil.spectro_gram(shifted, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "    input_loader = torch.utils.data.DataLoader(aug_sgram, batch_size=16, shuffle=False)\n",
    "    \n",
    "    for input in input_loader:\n",
    "        #print(input.shape)\n",
    "        input = input.reshape([-1,1,64,430]) #change 2nd number as u change number of channel\n",
    "        #print(input.shape)\n",
    "        input_m, input_s = input.mean(), input.std()\n",
    "        input = (input - input_m) / input_s\n",
    "        output = model(input)\n",
    "        _, prediction = torch.max(output,1)\n",
    "        prediction = prediction.numpy()[0]\n",
    "        #print(f\"I think this is the sound of a {sounds[prediction]}\")\n",
    "    return prediction \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think this is the sound of a Rooster\n",
      "This is actually the sound of a Rooster\n"
     ]
    }
   ],
   "source": [
    "file = r'../../Dataset/ESC-50-master/audio/5-200334-B-1.wav'\n",
    "real_label = int(file.split(\"-\")[-1].split(\".\")[0])\n",
    "prediction = endtoend(model, file)\n",
    "print(f\"I think this is the sound of a {sounds[prediction]}\")\n",
    "print(f\"This is actually the sound of a {sounds[real_label]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think this is the sound of a Dog\n"
     ]
    }
   ],
   "source": [
    "ifile = r'../../Internet Audio/dogbark_pcm24.wav'\n",
    "prediction_online = endtoend(model, ifile)\n",
    "print(f\"I think this is the sound of a {sounds[prediction_online]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(audio_dir, out_dir, audiofile):\n",
    "    audio_file = os.path.join(audio_dir, audiofile)\n",
    "    wave, sr = librosa.load(audio_file, sr=None) \n",
    "    frame_length = 5 #cut into 5s \n",
    "    segment_length = int(sr * frame_length)\n",
    "    num_sections = int(np.ceil(len(wave) / segment_length)) #the number of sections after splitting\n",
    "    split = []    \n",
    "    for s in range(0, len(wave), segment_length):\n",
    "        t = wave[s: s + segment_length]\n",
    "        split.append(t)\n",
    "    outfiles = []\n",
    "    recording_name = os.path.basename(audio_file[:-4])\n",
    "    for i, segment in enumerate(split):\n",
    "        out_file = f\"{recording_name}_{i}.wav\"\n",
    "        outfiles.append(out_file)\n",
    "        sf.write(os.path.join(out_dir, out_file), segment, sr)\n",
    "    splittedfiles = dict(zip(range(len(outfiles)), outfiles))\n",
    "    return splittedfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audioWithTime(model, audiofile):\n",
    "    audio_dir = r'../../Long Audio/original'\n",
    "    out_dir = r'../../Long Audio/split'\n",
    "    splittedfiles = split_audio(audio_dir, out_dir, audiofile)\n",
    "    #predictions = []\n",
    "    for f in splittedfiles:\n",
    "        prediction = endtoend(model,out_dir + \"/\" + splittedfiles[f])\n",
    "        print(f\"From {f*5}s to {(f+1)*5}s, it sounds like a {sounds[prediction]}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 0s to 5s, it sounds like a Door, wood creaks\n",
      "From 5s to 10s, it sounds like a Clock alarm\n",
      "From 10s to 15s, it sounds like a Clock alarm\n",
      "From 15s to 20s, it sounds like a Clapping\n",
      "From 20s to 25s, it sounds like a Mouse click\n",
      "From 25s to 30s, it sounds like a Coughing\n",
      "From 30s to 35s, it sounds like a Can opening\n"
     ]
    }
   ],
   "source": [
    "longfile = \"keyboard30s.wav\"\n",
    "audioWithTime(model,longfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('pythonGeneral')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5e820be2262a0fe2d430015389c1a2c1303250a56156485323cf96f38ca7d2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
