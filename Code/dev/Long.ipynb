{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my notebook to mess around with my own code <3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't touch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pydub\n",
    "import os \n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_label = [\"Dog\", \"Rooster\", \"Pig\", \"Cow\", \"Frog\", \"Cat\", \"Hen\", \"Insects (flying)\", \"Sheep\", \"Crow\"\n",
    "                ,\"Rain\", \"Sea waves\", \"Crackling fire\", \"Crickets\", \"Chirping birds\", \"Water drops\", \"Wind\", \"Pouring water\", \"Toilet flush\", \"Thunderstorm\"\n",
    "                ,\"Crying baby\", \"Sneezing\", \"Clapping\", \"Breathing\", \"Coughing\", \"Footsteps\", \"Laughing\", \"Brushing teeth\", \"Snoring\", \"Drinking, sipping\"\n",
    "                , \"Door knock\", \"Mouse click\", \"Keyboard typing\", \"Door, wood creaks\", \"Can opening\", \"washing machine\", \"Vacuum cleaner\", \"Clock alarm\", \"Clock tick\", \"Glass breaking\"\n",
    "                , \"Helicopter\", \"Chainsaw\", \"Siren\", \"Car horn\", \"Engine\", \"Train\", \"Church bells\", \"Airplane\", \"Fireworks\", \"Hand saw\"]\n",
    "sounds = dict(zip(range(50), sound_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Crow'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sounds[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "      <th>esc10</th>\n",
       "      <th>src_file</th>\n",
       "      <th>take</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>100032</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100038-A-14.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>chirping_birds</td>\n",
       "      <td>False</td>\n",
       "      <td>100038</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-100210-A-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-100210-B-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-101296-A-19.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>False</td>\n",
       "      <td>101296</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1-101296-B-19.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>False</td>\n",
       "      <td>101296</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1-101336-A-30.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>door_wood_knock</td>\n",
       "      <td>False</td>\n",
       "      <td>101336</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1-101404-A-34.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>can_opening</td>\n",
       "      <td>False</td>\n",
       "      <td>101404</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1-103298-A-9.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>crow</td>\n",
       "      <td>False</td>\n",
       "      <td>103298</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1-103995-A-30.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>door_wood_knock</td>\n",
       "      <td>False</td>\n",
       "      <td>103995</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  fold  target         category  esc10  src_file take\n",
       "0   1-100032-A-0.wav     1       0              dog   True    100032    A\n",
       "1  1-100038-A-14.wav     1      14   chirping_birds  False    100038    A\n",
       "2  1-100210-A-36.wav     1      36   vacuum_cleaner  False    100210    A\n",
       "3  1-100210-B-36.wav     1      36   vacuum_cleaner  False    100210    B\n",
       "4  1-101296-A-19.wav     1      19     thunderstorm  False    101296    A\n",
       "5  1-101296-B-19.wav     1      19     thunderstorm  False    101296    B\n",
       "6  1-101336-A-30.wav     1      30  door_wood_knock  False    101336    A\n",
       "7  1-101404-A-34.wav     1      34      can_opening  False    101404    A\n",
       "8   1-103298-A-9.wav     1       9             crow  False    103298    A\n",
       "9  1-103995-A-30.wav     1      30  door_wood_knock  False    103995    A"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../Dataset/ESC-50-master/meta/esc50.csv')\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_dir = r'../../Dataset/ESC-50-master/audio'\n",
    "\n",
    "# out_dir = r'../../Dataset/ESC-50-master/split_audio'\n",
    "# #os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# audio_file = os.path.join(audio_dir, '3-155556-A-31.wav')\n",
    "\n",
    "# wave, sr = librosa.load(audio_file, sr=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_length = 500/1000 #the lengh of the frame, I set it as 500ms for testing\n",
    "# segment_length = int(sr * frame_length)\n",
    "# #print(f\"sr {sr}\")\n",
    "# #print(segment_length)\n",
    "# num_sections = int(np.ceil(len(wave) / segment_length))  #the number of sections after splitting\n",
    "# #print(num_sections)\n",
    "# split = []    \n",
    "# for s in range(0, len(wave), segment_length):\n",
    "#     t = wave[s: s + segment_length]\n",
    "#     split.append(t)\n",
    "\n",
    "# recording_name = os.path.basename(audio_file[:-4])\n",
    "# for i, segment in enumerate(split):\n",
    "#     out_file = f\"{recording_name}_{i}.wav\"\n",
    "#     sf.write(os.path.join(out_dir, out_file), segment, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil():\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig,sr)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Standardizing sample rate to 44100Hz\n",
    "    # ----------------------------\n",
    "    def resample(audio, srate):\n",
    "        sig, sr = audio\n",
    "        if (sr == srate):\n",
    "            return audio\n",
    "        no_channels = sig.shape[0]\n",
    "\n",
    "        #Resample 1st channel:\n",
    "        resig = torchaudio.transforms.Resample(sr, srate)(sig[:1,:])\n",
    "        if (no_channels > 1):\n",
    "            #Resample 2nd channel and merge both\n",
    "            retwo = torchaudio.transforms.Resample(sr, srate)(sig[1:,:])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "\n",
    "        return ((resig, srate))\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "    # Some audios are mono, some are stereo. We need everything to have the same dimensions.\n",
    "    # Thus, we can either only select the first channel of stereo or duplicate the first channel of mono\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def rechannel(audio, channel):\n",
    "        sig, sr = audio\n",
    "        if (sig.shape[0]==channel):\n",
    "            return audio\n",
    "        if (channel==1):\n",
    "            resig = sig[:1,:]\n",
    "        else:\n",
    "            resig = torch.cat([sig,sig])\n",
    "\n",
    "        return ((resig, sr))\n",
    "\n",
    "    \n",
    "\n",
    "    # ----------------------------\n",
    "    # Standardize the length of the audio - that is, either pad or truncate the audio\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def resize_aud(audio, ms):\n",
    "        sig, sr = audio\n",
    "        no_rows, sig_len = sig.shape\n",
    "        max_len = sr // 1000 * ms\n",
    "\n",
    "        #Truncate\n",
    "        if (sig_len > max_len):\n",
    "            sig = sig[:, :max_len]\n",
    "        #Padding\n",
    "        elif (sig_len < max_len):\n",
    "            #Length of the paddings at the start and end of the signal\n",
    "            len_start = random.randint(0, max_len-sig_len)\n",
    "            len_end = max_len - len_start - sig_len\n",
    "\n",
    "            pad_start = torch.zeros((no_rows, len_start))\n",
    "            pad_end = torch.zeros((no_rows, len_end))\n",
    "\n",
    "            sig = torch.cat((pad_start, sig, pad_end), 1)\n",
    "\n",
    "        return (sig, sr)\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "    # Refer to textbox_1 for the reasoning of this method\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig,sr = aud\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Generating Spectrogram\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def spectro_gram(audio, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig, sr = audio\n",
    "        top_db = 80 #if we have more time, we can try 80\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "        #shape of spec is [channel (mono or stereo etc), n_mels, time]\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return (spec)\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "    # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
    "    # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
    "    # overfitting and to help the model generalise better. The masked sections are\n",
    "    # replaced with the mean value.\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        return aug_spec\n",
    "    \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textbox_1: We might also need to perform augmentation for raw data. \"Natural sounds\" such as traffic, sea waves, dog barks,.. usually have no particular order thus the audio could wrap around. On the other hand, human speech and alike sounds, the order matters. We can fill the gaps with silence. There are some options: time shift, pitch shift, time stretch, add noise,... We will try out the first option: time shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build a customized Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDS(Dataset):\n",
    "  def __init__(self, df, path):\n",
    "    self.df = df\n",
    "    self.path = str(path)\n",
    "    self.duration = 5000 #our audio is 5 seconds\n",
    "    self.sr = 44100\n",
    "    self.channel = 2\n",
    "    self.shift_pct = 0.4\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "  \n",
    "  def __shape__(self):\n",
    "    return self.df.shape\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    file = self.path + self.df.loc[index, 'filename']\n",
    "    class_id = self.df.loc[index, 'target'] #the index of the label aka target\n",
    "    fold = self.df.loc[index, 'fold']\n",
    "\n",
    "    audio = AudioUtil.open(file)\n",
    "    #print(f\"Original shape {audio[0].shape} and sample rate of {audio[1]}\")\n",
    "    rechannel = AudioUtil.rechannel(audio, self.channel)\n",
    "    #print(f\"Rechanneling shape {rechannel[0].shape} and sample rate of {rechannel[1]}\")\n",
    "    resamp = AudioUtil.resample(rechannel, self.sr)\n",
    "    #print(f\"Resampling shape {resamp[0].shape} and sample rate of {resamp[1]}\")\n",
    "    padded = AudioUtil.resize_aud(resamp, self.duration)\n",
    "    #print(f\"Padded shape {padded[0].shape} and sample rate of {padded[1]}\")\n",
    "    shifted = AudioUtil.time_shift(padded, self.shift_pct)\n",
    "    #print(f\"Time shift shape {shifted[0].shape} and sample rate of {shifted[1]}\")\n",
    "    sgram = AudioUtil.spectro_gram(shifted, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    #print(f\"Mel spectrogram shape {sgram.shape}\")\n",
    "    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "    #print(f\"Augmented spectrogram shape {aug_sgram.shape} of (num_channels, Mel freq_bands, time_steps)\")\n",
    "    return aug_sgram, class_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'../../Dataset/ESC-50-master/audio/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting for SVM. If you run CNN, ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.Subset'> 1600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 32\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(train_ds), \u001b[38;5;28mlen\u001b[39m(train_ds))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Create training and validation data loaders\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# train_dl = torch.utils.data.DataLoader(data_train, batch_size=16, shuffle=True)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# test_dl = torch.utils.data.DataLoader(test_ds, batch_size=16, shuffle=False)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# torch.save(test_dl, save_path + f'X_test_{i+1}.npy')\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# torch.save(val_dl, save_path + f'X_val_{i+1}.npy')\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_train_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m np\u001b[38;5;241m.\u001b[39msave(save_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, test_ds)\n\u001b[1;32m     34\u001b[0m np\u001b[38;5;241m.\u001b[39msave(save_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_val_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, val_ds)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/lib/npyio.py:521\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    518\u001b[0m     file_ctx \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(file, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m \u001b[39mwith\u001b[39;00m file_ctx \u001b[39mas\u001b[39;00m fid:\n\u001b[0;32m--> 521\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masanyarray(arr)\n\u001b[1;32m    522\u001b[0m     \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mwrite_array(fid, arr, allow_pickle\u001b[39m=\u001b[39mallow_pickle,\n\u001b[1;32m    523\u001b[0m                        pickle_kwargs\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(fix_imports\u001b[39m=\u001b[39mfix_imports))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "Cell \u001b[0;32mIn [23], line 32\u001b[0m, in \u001b[0;36mSoundDS.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     30\u001b[0m shifted \u001b[38;5;241m=\u001b[39m AudioUtil\u001b[38;5;241m.\u001b[39mtime_shift(padded, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshift_pct)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#print(f\"Time shift shape {shifted[0].shape} and sample rate of {shifted[1]}\")\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m sgram \u001b[38;5;241m=\u001b[39m \u001b[43mAudioUtil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectro_gram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshifted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#print(f\"Mel spectrogram shape {sgram.shape}\")\u001b[39;00m\n\u001b[1;32m     34\u001b[0m aug_sgram \u001b[38;5;241m=\u001b[39m AudioUtil\u001b[38;5;241m.\u001b[39mspectro_augment(sgram, max_mask_pct\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, n_freq_masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_time_masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn [22], line 87\u001b[0m, in \u001b[0;36mAudioUtil.spectro_gram\u001b[0;34m(audio, n_mels, n_fft, hop_len)\u001b[0m\n\u001b[1;32m     85\u001b[0m sig, sr \u001b[38;5;241m=\u001b[39m audio\n\u001b[1;32m     86\u001b[0m top_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m \u001b[38;5;66;03m#if we have more time, we can try 80\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m spec \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMelSpectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_mels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#shape of spec is [channel (mono or stereo etc), n_mels, time]\u001b[39;00m\n\u001b[1;32m     89\u001b[0m spec \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mAmplitudeToDB(top_db\u001b[38;5;241m=\u001b[39mtop_db)(spec)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:642\u001b[0m, in \u001b[0;36mMelSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, waveform: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    635\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[39m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39m        Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     specgram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspectrogram(waveform)\n\u001b[1;32m    643\u001b[0m     mel_specgram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmel_scale(specgram)\n\u001b[1;32m    644\u001b[0m     \u001b[39mreturn\u001b[39;00m mel_specgram\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:108\u001b[0m, in \u001b[0;36mSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, waveform: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     99\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39m        Fourier bins, and time is the number of window hops (n_frame).\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mspectrogram(\n\u001b[1;32m    109\u001b[0m         waveform,\n\u001b[1;32m    110\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad,\n\u001b[1;32m    111\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow,\n\u001b[1;32m    112\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_fft,\n\u001b[1;32m    113\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhop_length,\n\u001b[1;32m    114\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwin_length,\n\u001b[1;32m    115\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpower,\n\u001b[1;32m    116\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized,\n\u001b[1;32m    117\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcenter,\n\u001b[1;32m    118\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_mode,\n\u001b[1;32m    119\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monesided,\n\u001b[1;32m    120\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchaudio/functional/functional.py:121\u001b[0m, in \u001b[0;36mspectrogram\u001b[0;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[0m\n\u001b[1;32m    118\u001b[0m waveform \u001b[39m=\u001b[39m waveform\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    120\u001b[0m \u001b[39m# default values are consistent with librosa.core.spectrum._spectrogram\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m spec_f \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstft(\n\u001b[1;32m    122\u001b[0m     \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mwaveform,\n\u001b[1;32m    123\u001b[0m     n_fft\u001b[39m=\u001b[39;49mn_fft,\n\u001b[1;32m    124\u001b[0m     hop_length\u001b[39m=\u001b[39;49mhop_length,\n\u001b[1;32m    125\u001b[0m     win_length\u001b[39m=\u001b[39;49mwin_length,\n\u001b[1;32m    126\u001b[0m     window\u001b[39m=\u001b[39;49mwindow,\n\u001b[1;32m    127\u001b[0m     center\u001b[39m=\u001b[39;49mcenter,\n\u001b[1;32m    128\u001b[0m     pad_mode\u001b[39m=\u001b[39;49mpad_mode,\n\u001b[1;32m    129\u001b[0m     normalized\u001b[39m=\u001b[39;49mframe_length_norm,\n\u001b[1;32m    130\u001b[0m     onesided\u001b[39m=\u001b[39;49monesided,\n\u001b[1;32m    131\u001b[0m     return_complex\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    132\u001b[0m )\n\u001b[1;32m    134\u001b[0m \u001b[39m# unpack batch\u001b[39;00m\n\u001b[1;32m    135\u001b[0m spec_f \u001b[39m=\u001b[39m spec_f\u001b[39m.\u001b[39mreshape(shape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m spec_f\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/functional.py:632\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[1;32m    631\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mview(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39msignal_dim:])\n\u001b[0;32m--> 632\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mstft(\u001b[39minput\u001b[39;49m, n_fft, hop_length, win_length, window,  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m    633\u001b[0m                 normalized, onesided, return_complex)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_path = r'../../Dataset/ESC-50-master/audio/'\n",
    "save_path = r'../../Preprocessed Data/'\n",
    "np.random.seed(138)\n",
    "data = SoundDS(df, data_path)\n",
    "for i in range(5):\n",
    "    # df_train = df[df.fold != (i+1)]\n",
    "    # df_test = df[df.fold == (i+1)]\n",
    "    #data_train = SoundDS(df, data_path)\n",
    "\n",
    "    #print(type(data_train))\n",
    "    #data_test = SoundDS(df_test, data_path)\n",
    "    #print(\"Data train\", data_train.__shape__())\n",
    "    # Random split of 80:10:10 between testing and validating\n",
    "    num_items = len(data)\n",
    "    num_train = round(num_items * 0.8)\n",
    "    num_val = num_items - num_train\n",
    "    num_test = round(num_val * 0.5)\n",
    "    num_val_2 = num_val - num_test\n",
    "    train_ds, val = random_split(data, [num_train, num_val])\n",
    "    val_ds, test_ds = random_split(val, [num_val_2, num_test])\n",
    "    print(type(train_ds), len(train_ds))\n",
    "\n",
    "    # Create training and validation data loaders\n",
    "    # train_dl = torch.utils.data.DataLoader(data_train, batch_size=16, shuffle=True)\n",
    "    # test_dl = torch.utils.data.DataLoader(test_ds, batch_size=16, shuffle=False)\n",
    "    # val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "    # print(train_dl)\n",
    "    # torch.save(train_dl, save_path + f'X_train_{i+1}.npy')\n",
    "    # torch.save(test_dl, save_path + f'X_test_{i+1}.npy')\n",
    "    # torch.save(val_dl, save_path + f'X_val_{i+1}.npy')\n",
    "\n",
    "    np.save(save_path + f'X_train_{i+1}.npy', train_ds)\n",
    "    np.save(save_path + f'X_test_{i+1}.npy', test_ds)\n",
    "    np.save(save_path + f'X_val_{i+1}.npy', val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.load(r'../../Preprocessed Data/X_train_1.npy', allow_pickle=True)\n",
    "# testdf = pd.DataFrame(test, columns = [\"Tensor\", \"Target\"])\n",
    "# testdf.Tensor[0].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out a CNN model online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "myds = SoundDS(df, data_path)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class AudioClassifier (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Third Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Fourth Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=50)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't change any parameters, you can skip this and just use our pre-trained models under Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 3.89, Accuracy: 0.02\n",
      "Epoch: 1, Loss: 3.81, Accuracy: 0.05\n",
      "Epoch: 2, Loss: 3.72, Accuracy: 0.07\n",
      "Epoch: 3, Loss: 3.62, Accuracy: 0.08\n",
      "Epoch: 4, Loss: 3.54, Accuracy: 0.09\n",
      "Epoch: 5, Loss: 3.47, Accuracy: 0.12\n",
      "Epoch: 6, Loss: 3.40, Accuracy: 0.12\n",
      "Epoch: 7, Loss: 3.35, Accuracy: 0.13\n",
      "Epoch: 8, Loss: 3.28, Accuracy: 0.16\n",
      "Epoch: 9, Loss: 3.20, Accuracy: 0.17\n",
      "Epoch: 10, Loss: 3.11, Accuracy: 0.21\n",
      "Epoch: 11, Loss: 3.03, Accuracy: 0.20\n",
      "Epoch: 12, Loss: 2.95, Accuracy: 0.22\n",
      "Epoch: 13, Loss: 2.88, Accuracy: 0.24\n",
      "Epoch: 14, Loss: 2.80, Accuracy: 0.25\n",
      "Epoch: 15, Loss: 2.74, Accuracy: 0.26\n",
      "Epoch: 16, Loss: 2.66, Accuracy: 0.30\n",
      "Epoch: 17, Loss: 2.57, Accuracy: 0.33\n",
      "Epoch: 18, Loss: 2.50, Accuracy: 0.32\n",
      "Epoch: 19, Loss: 2.42, Accuracy: 0.36\n",
      "Epoch: 20, Loss: 2.39, Accuracy: 0.36\n",
      "Epoch: 21, Loss: 2.32, Accuracy: 0.37\n",
      "Epoch: 22, Loss: 2.28, Accuracy: 0.39\n",
      "Epoch: 23, Loss: 2.20, Accuracy: 0.39\n",
      "Epoch: 24, Loss: 2.12, Accuracy: 0.43\n",
      "Epoch: 25, Loss: 2.10, Accuracy: 0.42\n",
      "Epoch: 26, Loss: 2.02, Accuracy: 0.45\n",
      "Epoch: 27, Loss: 1.98, Accuracy: 0.45\n",
      "Epoch: 28, Loss: 1.97, Accuracy: 0.46\n",
      "Epoch: 29, Loss: 1.85, Accuracy: 0.48\n",
      "Epoch: 30, Loss: 1.82, Accuracy: 0.49\n",
      "Epoch: 31, Loss: 1.76, Accuracy: 0.51\n",
      "Epoch: 32, Loss: 1.70, Accuracy: 0.54\n",
      "Epoch: 33, Loss: 1.70, Accuracy: 0.51\n",
      "Epoch: 34, Loss: 1.64, Accuracy: 0.53\n",
      "Epoch: 35, Loss: 1.60, Accuracy: 0.56\n",
      "Epoch: 36, Loss: 1.58, Accuracy: 0.54\n",
      "Epoch: 37, Loss: 1.57, Accuracy: 0.55\n",
      "Epoch: 38, Loss: 1.50, Accuracy: 0.57\n",
      "Epoch: 39, Loss: 1.44, Accuracy: 0.59\n",
      "Epoch: 40, Loss: 1.40, Accuracy: 0.61\n",
      "Epoch: 41, Loss: 1.40, Accuracy: 0.60\n",
      "Epoch: 42, Loss: 1.32, Accuracy: 0.62\n",
      "Epoch: 43, Loss: 1.36, Accuracy: 0.60\n",
      "Epoch: 44, Loss: 1.32, Accuracy: 0.62\n",
      "Epoch: 45, Loss: 1.31, Accuracy: 0.63\n",
      "Epoch: 46, Loss: 1.26, Accuracy: 0.64\n",
      "Epoch: 47, Loss: 1.26, Accuracy: 0.63\n",
      "Epoch: 48, Loss: 1.23, Accuracy: 0.66\n",
      "Epoch: 49, Loss: 1.17, Accuracy: 0.67\n",
      "Epoch: 50, Loss: 1.20, Accuracy: 0.65\n",
      "Epoch: 51, Loss: 1.14, Accuracy: 0.68\n",
      "Epoch: 52, Loss: 1.16, Accuracy: 0.67\n",
      "Epoch: 53, Loss: 1.09, Accuracy: 0.70\n",
      "Epoch: 54, Loss: 1.13, Accuracy: 0.67\n",
      "Epoch: 55, Loss: 1.10, Accuracy: 0.68\n",
      "Epoch: 56, Loss: 1.09, Accuracy: 0.68\n",
      "Epoch: 57, Loss: 1.06, Accuracy: 0.70\n",
      "Epoch: 58, Loss: 1.05, Accuracy: 0.71\n",
      "Epoch: 59, Loss: 1.03, Accuracy: 0.71\n",
      "Epoch: 60, Loss: 1.04, Accuracy: 0.71\n",
      "Epoch: 61, Loss: 1.02, Accuracy: 0.70\n",
      "Epoch: 62, Loss: 1.01, Accuracy: 0.70\n",
      "Epoch: 63, Loss: 0.97, Accuracy: 0.72\n",
      "Epoch: 64, Loss: 0.99, Accuracy: 0.71\n",
      "Epoch: 65, Loss: 0.96, Accuracy: 0.73\n",
      "Epoch: 66, Loss: 0.90, Accuracy: 0.74\n",
      "Epoch: 67, Loss: 0.95, Accuracy: 0.73\n",
      "Epoch: 68, Loss: 0.93, Accuracy: 0.74\n",
      "Epoch: 69, Loss: 0.91, Accuracy: 0.73\n",
      "Epoch: 70, Loss: 0.91, Accuracy: 0.73\n",
      "Epoch: 71, Loss: 0.90, Accuracy: 0.76\n",
      "Epoch: 72, Loss: 0.90, Accuracy: 0.75\n",
      "Epoch: 73, Loss: 0.85, Accuracy: 0.75\n",
      "Epoch: 74, Loss: 0.85, Accuracy: 0.76\n",
      "Epoch: 75, Loss: 0.86, Accuracy: 0.75\n",
      "Epoch: 76, Loss: 0.84, Accuracy: 0.76\n",
      "Epoch: 77, Loss: 0.87, Accuracy: 0.76\n",
      "Epoch: 78, Loss: 0.83, Accuracy: 0.77\n",
      "Epoch: 79, Loss: 0.82, Accuracy: 0.77\n",
      "Epoch: 80, Loss: 0.82, Accuracy: 0.77\n",
      "Epoch: 81, Loss: 0.81, Accuracy: 0.76\n",
      "Epoch: 82, Loss: 0.84, Accuracy: 0.75\n",
      "Epoch: 83, Loss: 0.82, Accuracy: 0.76\n",
      "Epoch: 84, Loss: 0.77, Accuracy: 0.80\n",
      "Epoch: 85, Loss: 0.79, Accuracy: 0.78\n",
      "Epoch: 86, Loss: 0.83, Accuracy: 0.77\n",
      "Epoch: 87, Loss: 0.78, Accuracy: 0.79\n",
      "Epoch: 88, Loss: 0.78, Accuracy: 0.78\n",
      "Epoch: 89, Loss: 0.78, Accuracy: 0.77\n",
      "Epoch: 90, Loss: 0.79, Accuracy: 0.78\n",
      "Epoch: 91, Loss: 0.78, Accuracy: 0.77\n",
      "Epoch: 92, Loss: 0.77, Accuracy: 0.77\n",
      "Epoch: 93, Loss: 0.76, Accuracy: 0.79\n",
      "Epoch: 94, Loss: 0.74, Accuracy: 0.80\n",
      "Epoch: 95, Loss: 0.76, Accuracy: 0.79\n",
      "Epoch: 96, Loss: 0.74, Accuracy: 0.80\n",
      "Epoch: 97, Loss: 0.75, Accuracy: 0.78\n",
      "Epoch: 98, Loss: 0.79, Accuracy: 0.77\n",
      "Epoch: 99, Loss: 0.75, Accuracy: 0.79\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def training(model, train_dl, num_epochs):\n",
    "  # Loss Function, Optimizer and Scheduler\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "  # Repeat for each epoch\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Repeat for each batch in the training set\n",
    "    for i, data in enumerate(train_dl):\n",
    "        # Get the input features and target labels, and put them on the GPU\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Normalize the inputs\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Keep stats for Loss and Accuracy\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "        # Get the predicted class with the highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        # Count of predictions that matched the target label\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "\n",
    "        #if i % 10 == 0:    # print every 10 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "    \n",
    "    # Print stats at the end of the epoch\n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "  print('Finished Training')\n",
    "  \n",
    "num_epochs=100   # Just for demo, adjust this higher.\n",
    "training(myModel, train_dl, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model trained with 100 epochs on 80 db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../Models/cnn-100.pt'\n",
    "torch.save(myModel.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62, Total items: 400\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Inference\n",
    "# ----------------------------\n",
    "def inference (model, val_dl):\n",
    "  correct_prediction = 0\n",
    "  total_prediction = 0\n",
    "\n",
    "  # Disable gradient updates\n",
    "  with torch.no_grad():\n",
    "    for data in val_dl:\n",
    "      # Get the input features and target labels, and put them on the GPU\n",
    "      inputs, labels = data[0].to(device), data[1].to(device)\n",
    "      #print(inputs.shape)\n",
    "      # Normalize the inputs\n",
    "      inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "      inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "      # Get predictions\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      # Get the predicted class with the highest score\n",
    "      _, prediction = torch.max(outputs,1)\n",
    "      # Count of predictions that matched the target label\n",
    "      correct_prediction += (prediction == labels).sum().item()\n",
    "      total_prediction += prediction.shape[0]\n",
    "    \n",
    "  acc = correct_prediction/total_prediction\n",
    "  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
    "\n",
    "# Run inference on trained model with the validation set\n",
    "inference(myModel, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run 100 epochs in 22 min with accuracy 0.75\n",
    "\n",
    "\n",
    "test accuracy 0.53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"End to end\" methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioClassifier(\n",
       "  (conv1): Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (relu1): ReLU()\n",
       "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (relu3): ReLU()\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (relu4): ReLU()\n",
       "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "  (lin): Linear(in_features=64, out_features=50, bias=True)\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = r'../../Models/cnn-100.pt'\n",
    "#device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioClassifier()\n",
    "model.load_state_dict(torch.load(PATH,map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper funciton to find the most frequent number in a list\n",
    "def most_frequent(listA):\n",
    "    res = max(set(listA), key = listA.count)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def endtoend(model, audiofile):\n",
    "    audio = AudioUtil.open(audiofile)\n",
    "    rechannel = AudioUtil.rechannel(audio, 2)\n",
    "    resamp = AudioUtil.resample(rechannel, 44100)\n",
    "\n",
    "    padded = AudioUtil.resize_aud(resamp, 5000)\n",
    "    shifted = AudioUtil.time_shift(padded, 0.4)\n",
    "    sgram = AudioUtil.spectro_gram(shifted, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "    input_loader = torch.utils.data.DataLoader(aug_sgram, batch_size=16, shuffle=False)\n",
    "    \n",
    "    for input in input_loader:\n",
    "        #print(input.shape)\n",
    "        input = input.reshape([-1,2,64,430])\n",
    "        #print(input.shape)\n",
    "        input_m, input_s = input.mean(), input.std()\n",
    "        input = (input - input_m) / input_s\n",
    "        output = model(input)\n",
    "        _, prediction = torch.max(output,1)\n",
    "        prediction = prediction.numpy()[0]\n",
    "        print(f\"I think this is the sound of a {sounds[prediction]}\")\n",
    "    return prediction \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think this is the sound of a Rooster\n",
      "This is actually the sound of a Rooster\n"
     ]
    }
   ],
   "source": [
    "file = r'../../Dataset/ESC-50-master/audio/5-200334-B-1.wav'\n",
    "real_label = int(file.split(\"-\")[-1].split(\".\")[0])\n",
    "endtoend(model, file)\n",
    "print(f\"This is actually the sound of a {sounds[real_label]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good ones: \n",
    "\n",
    "- Usually are loud and clear noises\n",
    "\n",
    "- For animals (3,4,6,..) sounds, the accuracy is very good and the CNN model is able to predict them correctly most of the time\n",
    "\n",
    "- Hand saw, church bells\n",
    "\n",
    "## Bad ones:\n",
    "\n",
    "- Clock alarm vs glass breaking\n",
    "\n",
    "- Clock alarms water drops\n",
    "\n",
    "- Coughing vs water drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
